\section{Introduction}
\label{sec:introduction}
Detecting faulty modules before releasing projects is crucial to software industry. Faulty code contains specific bugs, e.g. division by zero, infinite loops and buffer overflow that make programs work incorrectly such as out of memory, execution interruption, or generating the unexpected output. It is infeasible to prevent all bugs in software components due to various causes including programmer's skill, miscommunication, buggy third-party tools and so on. According to annual reports, software bugs have led to many devastating consequences in terms of the loss of finance, company reputation and customer satisfaction \cite{laradji2015software}. The later the bugs are detected in the development process, the more serious the consequences are. For above reasons, faulty code detection (FCD) is a hot topic for both academia and industry in the field of software engineering.

There are two major approaches to build machine learning models for detecting faulty modules. The traditional methods design software metrics, e.g. function points and the complexity to estimate the characteristics of programs and then train detectors based on such features \cite{chidamber1994metrics, lorenz1994object, curtis1979measuring}. The main obstacles are extracting good features that are highly relevant to software bugs. Software bugs depend on specific scenarios and only appear when satisfying certain conditions. Thus, the surface features are difficult to capture them. For example, given two C statements \texttt{scanf("\%s",string)} and  \texttt{scanf("\%10s",string)} where \texttt{string} is a character array, the first one potentially contains a buffer overflow bug due to not verifying the input sequence length, while the second one is safe. Recently, investigating deep neural networks to automatically generate sophisticated features from different program representations has made many breakthroughs in program analysis problems. Learning syntactic structures on abstract syntax trees (ASTs) is beneficial to program classification \cite{phan2017sibstcnn, mou2016convolutional}. For faulty code prediction, current studies have focused on both representations of ASTs and assembly instructions to develop deep models \cite{phan2017convolutional, li2017software}. According to several reports, deep models on assembly instructions can achieve higher performance because they are closed to machine code and show the execution process of programs \cite{phan2017convolutional, phan2017conv_asm}.    

However, both the approaches have been suffered from the imbalance and lack of labelled data. Indeed, collecting large amount of historical source code from software projects is not trivial due to confidentiality. Moreover, the number of clean modules normally dominates that of defective modules while detecting the defective ones is more preferable. As a proof, the software metrics datasets in PROMISE repository \footnote{\url{http://promise.site.uottawa.ca/SERepository/}} \cite{Sayyad-Shirabad+Menzies:2005} just contain hundreds to several thousands instances including the duplication and the defective ratios range from 7\% to 33\%. Similarly, open source projects like camel (enterprise integration framework) and jEdit (text editor designed for programmers) were utilized in several researches have only hundred files \cite{li2017software}.

Many efforts have been made to overcome the data shortage problem in the software engineering field. Some works leverage the data from previous versions in building models to detect faulty modules in the new version\cite{wu2018cross}. These methods result in the over-fitting problem since many instances may be available in both training and test sets. Another selection is the transfer learning technique that uses the data from other projects to pre-train and then fine-tune the model with the current project data. Reported by many works, transfer learning is an efficient solution to tackle the limited data problem. This is one of the keys to bring deep neural networks to the real world applications.

This paper proposes a new deep neural network that is able to learn with smaller amount of labelled data to overcome the data shortage problem. The model has two sub-networks performing different tasks including an autoencoder to generate program latent representations and a classifier for prediction. The sub-networks share two some first layers of convolution that undertakes encoding programs for the autoencoder and extracting features for the classifier. The motivation for building the joint model comes from the advantages of the autoencoder. In the reduction side, the latent representations contains semantic features such that the decoder side can reconstruct input programs. These semantic features are beneficial to distinguish programs. We also propose a learning strategy using semi-supervised learning to increase the performance of the model. The autoencoder branch is trained by unsupervised manner. Then, we continue training the whole model with labelled data instead of from scratch. This is feasible in practice because unlabelled data can be collected from a large amount of open source files in the internet. 

In  summary, this paper makes the following contributions:
\begin{itemize}
    \item Designing a new multitask network for faulty code detection problem.
    \item Formulation of training the network by semi-supervised manner to reduce the labelled data requirement.
    \item Comprehensively analyzing the influence of the number of training samples on the model.
\end{itemize}{}{}

The remainder is organized as follows: Section \ref{sec:related_work} surveys studies related to faulty code prediction and the solutions to deal with data shortage and imbalance problems. The proposed model architecture and semi-supervised training strategy are described in Section \ref{sec:proposed_approach}. Section \ref{sec:experiments} presents the models' settings for conducting experiments. Section \ref{sec:results} analyzes and discusses  the  experimental  results. Section \ref{sec:conclusion} concludes the new findings in this work.